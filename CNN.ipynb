{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZtnSsSdJXLg"
      },
      "outputs": [],
      "source": [
        "#Secventa de incarcare si dezarhivare a imaginilor\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# !unzip  gdrive/My\\ Drive/unibuc-brain-ad.zip\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "#functia care imi proceseaza imaginile, in functie de ce lungime are numarul imaginii, stiu cati de 0 se afla\n",
        "#in numele png-ului. folosesc convert(\"L\") pentru a converti imaginile la formatul alb-negru\n",
        "def process_images(start, end):\n",
        "  lst = []\n",
        "  for i in range(start,end):\n",
        "    if len(str(i)) == 1:\n",
        "      img = Image.open(\"data/data/00000\" + str(i) + \".png\").convert(\"L\")\n",
        "    elif len(str(i)) == 2:\n",
        "      img = Image.open(\"data/data/0000\" + str(i) + \".png\").convert(\"L\")\n",
        "    elif len(str(i)) == 3:\n",
        "      img = Image.open(\"data/data/000\" + str(i) + \".png\").convert(\"L\")\n",
        "    elif len(str(i)) == 4:\n",
        "      img = Image.open(\"data/data/00\" + str(i) + \".png\").convert(\"L\")\n",
        "    elif len(str(i)) == 5:\n",
        "      img = Image.open(\"data/data/0\" + str(i) + \".png\").convert(\"L\")\n",
        "    img = np.array(img)\n",
        "    #micsorarea dimensiunii imaginii\n",
        "    img = img[18:194]\n",
        "    img = img[:, 30:190]\n",
        "    #normalizarea imaginii\n",
        "    normalized_image = preprocessing.normalize(img, norm = \"l2\")\n",
        "    lst.append(normalized_image)\n",
        "  return lst\n",
        "\n",
        "#creare imaginile de antrenare, care stiu ca sunt de la 1 la 15000\n",
        "#imaginilor le fac reshape pentru a le putea da ca parametru modelului de CNN. 1 este canalul de culoare\n",
        "#deoarece imaginile sunt alb-negru\n",
        "\n",
        "test_images = np.array(process_images(17001,22150))\n",
        "test_images = test_images.reshape(5149, 176, 160, 1)\n",
        "\n",
        "#creare imaginile de validare, in numar de 2000, de la 15001 la 17000train_images = np.array(process_images(1,15001))\n",
        "train_images = train_images.reshape(15000, 176, 160, 1)\n",
        "\n",
        "#creare imaginile de testare, 5149 care au mai ramas\n",
        "validation_images = np.array(process_images(15001,17001))\n",
        "validation_images = validation_images.reshape(2000, 176, 160, 1)\n",
        "\n",
        "#procesarea label-urilor\n",
        "def process_labels(start, end, file):\n",
        "    #imi initializez astfel label-urile deoarece nu sunt unidimensionale\n",
        "    #daca o imagine apartine clasei 0, label-ul sau este [1,0], daca apartine clasei 1, label-ul este [0,1]\n",
        "  labels = np.array([np.array([0,0]) for i in range(start,end)])\n",
        "  with open(file, 'r') as fin:\n",
        "    fin.readline()\n",
        "    for i in range(start,end):\n",
        "      line = [word for word in fin.readline().strip().split(\",\")]\n",
        "      labels[i][int(line[1])] = 1\n",
        "  return labels\n",
        "\n",
        "#crearea label-urilor\n",
        "train_labels = process_labels(0,15000,'/content/data/train_labels.txt')\n",
        "validation_labels = process_labels(0,2000, '/content/data/validation_labels.txt')\n",
        "\n",
        "#in model, pe langa straturile tipice convolutionale si de MaxPol2D, am si straturi de BatchNormalization si\n",
        "#Dropout pentru a evita overfitting-ul\n",
        "def creare_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "      layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      layers.Dropout(0.1),\n",
        "\n",
        "      layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu'),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      layers.Dropout(0.1),\n",
        "\n",
        "      layers.Conv2D(filters=512, kernel_size=(3,3), activation='relu'),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      layers.Dropout(0.2),\n",
        "\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(512, activation='relu'),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(256, activation='relu'),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(128, activation='relu'),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(2, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "numar = 0\n",
        "total_corect = 0\n",
        "scor_anterior = 0\n",
        "fisiere = []\n",
        "best_scores = []\n",
        "count = 0\n",
        "\n",
        "#functia de salvare a celor 5 modele cele mai performante\n",
        "class Save_Best(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        global numar\n",
        "        global test_images\n",
        "        global total_corect\n",
        "        global scor_anterior\n",
        "        global fisiere\n",
        "        global best_scores\n",
        "        global count\n",
        "        #obtin predictiile pentru imaginile de test\n",
        "        predictions = self.model.predict(test_images)\n",
        "        #obtin predictiile pentru imaginile de validare\n",
        "        predictions_validation = self.model.predict(validation_images)\n",
        "        #obtin acuratetea si scorul AUC\n",
        "        evaluare = self.model.evaluate(validation_images, validation_labels)\n",
        "        #calculez matricea de confuzie si o si afisez\n",
        "        conf_matrix = confusion_matrix([np.argmax(pred) for pred in validation_labels], [np.argmax(pred) for pred in predictions_validation])\n",
        "        print(conf_matrix)\n",
        "\n",
        "        #calculez cati de 1 am in predictiile pentru imaginile de test\n",
        "        counter = 0\n",
        "        for prediction in predictions:\n",
        "          if np.argmax(prediction) == 1:\n",
        "            counter += 1\n",
        "        print(counter)\n",
        "\n",
        "        #in aceste conditii, salvez un model\n",
        "        if conf_matrix[1][1] >= 100 and evaluare[2] >= 0.9:\n",
        "            #daca inca nu am 5 modele salvate, il salvez oricum\n",
        "            if len(best_scores) < 5:\n",
        "              count += 1\n",
        "              nume_fisier = \"model\" + str(count) + \".h5\"\n",
        "              self.model.save(nume_fisier)\n",
        "              #updatez lista cu fisiere si lista cu cele mai bune scoruri\n",
        "              fisiere.append(nume_fisier)\n",
        "              best_scores.append(evaluare[2])\n",
        "              print(f'Salvat model cu {counter} de 1 in output.')\n",
        "            else:\n",
        "                #daca deja am salvat 5 modele,\n",
        "                #in acest caz salvez un model doar daca este mai bun decat cel mai slab model salvat deja\n",
        "              if evaluare[2] > min(best_scores):\n",
        "                count += 1\n",
        "                #gasesc indexul minimului\n",
        "                index = best_scores.index(min(best_scores))\n",
        "                #inlocuiesc scorurile si numele fisierului de la acea pozitie\n",
        "                best_scores[index] = evaluare[2]\n",
        "                nume_fisier = \"model\" + str(count) +\".h5\"\n",
        "                fisiere[index] = nume_fisier\n",
        "                self.model.save(nume_fisier)\n",
        "                print(f'Salvat model cu {counter} de 1 in output.')\n",
        "\n",
        "model_curent = creare_model()\n",
        "model_curent.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[AUC(), 'accuracy'])\n",
        "\n",
        "\n",
        "#antrenez pentru 50 de epoci\n",
        "model_curent.fit(train_images, train_labels, epochs=50, callbacks=[Save_Best()])\n",
        "\n",
        "#functia de incarcare a celor 5 modele salvate\n",
        "def load_models(fisiere):\n",
        "  total_predictii = []\n",
        "  for fisier in fisiere:\n",
        "      #incarcare model\n",
        "    model_curent.load_weights(fisier)\n",
        "    predictii_test = [np.argmax(pred) for pred in model_curent.predict(test_images)]\n",
        "      #obtin predictiile pe setul de validare pentru a calcula matricea de confuzie pentru acest model\n",
        "    predictions = model_curent.predict(validation_images)\n",
        "    display_matrice = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix([np.argmax(pred) for pred in validation_labels], [np.argmax(pred) for pred in predictions]), display_labels = [0,1])\n",
        "    display_matrice.plot()\n",
        "    plt.show()\n",
        "\n",
        "    #afisez recall si precizia pentru modelul curent, pentru ambele clase\n",
        "    print(metrics.precision_score([np.argmax(pred) for pred in validation_labels], [np.argmax(pred) for pred in predictions], pos_label=0))\n",
        "    print(metrics.recall_score([np.argmax(pred) for pred in validation_labels], [np.argmax(pred) for pred in predictions],pos_label=0))\n",
        "    print(metrics.precision_score([np.argmax(pred) for pred in validation_labels], [np.argmax(pred) for pred in predictions], pos_label=1))\n",
        "    print(metrics.recall_score([np.argmax(pred) for pred in validation_labels], [np.argmax(pred) for pred in predictions], pos_label=1))\n",
        "    #adaug la predictiile totale predictiile modelului curent\n",
        "    total_predictii.append(predictii_test)\n",
        "  return total_predictii\n",
        "\n",
        "total_predictii = load_models(fisiere)\n",
        "\n",
        "#calculez predictiile finale prin suma predictiilor de pe fiecare pozitie\n",
        "suma_pe_coloane = np.sum(total_predictii, axis=0)\n",
        "\n",
        "#crearea fisierului de output\n",
        "def create_output_file():\n",
        "  with open('/content/data/submission.csv', 'w') as fout:\n",
        "    print(\"id,class\", file = fout)\n",
        "    image_idx = 17001\n",
        "    for prediction in suma_pe_coloane:\n",
        "        #daca macar un model a prezis ca o imagine apartine clasei 1, atunci o incadrez in clasa 1\n",
        "      if prediction >= 1:\n",
        "        print(\"0\"+str(image_idx)+\",1\", file = fout)\n",
        "      else:\n",
        "        print(\"0\"+str(image_idx)+\",0\", file = fout)\n",
        "      image_idx += 1\n",
        "\n",
        "create_output_file()"
      ]
    }
  ]
}