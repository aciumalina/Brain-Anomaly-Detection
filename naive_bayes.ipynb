{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZtnSsSdJXLg"
      },
      "outputs": [],
      "source": [
        "#Secventa de incarcare si dezarhivare a imaginilor\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# !unzip  gdrive/My\\ Drive/unibuc-brain-ad.zip\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#functia care imi proceseaza imaginile, in functie de ce lungime are numarul imaginii, stiu cati de 0 se afla\n",
        "#in numele png-ului. folosesc convert(\"L\") pentru a converti imaginile la formatul alb-negru\n",
        "def process_images(start, end):\n",
        "  lst = []\n",
        "  for i in range(start,end):\n",
        "    if len(str(i)) == 1:\n",
        "      img = Image.open(\"data/data/00000\" + str(i) + \".png\").convert(\"L\")\n",
        "    elif len(str(i)) == 2:\n",
        "      img = Image.open(\"data/data/0000\" + str(i) + \".png\").convert(\"L\")\n",
        "    elif len(str(i)) == 3:\n",
        "      img = Image.open(\"data/data/000\" + str(i) + \".png\").convert(\"L\")\n",
        "    elif len(str(i)) == 4:\n",
        "      img = Image.open(\"data/data/00\" + str(i) + \".png\").convert(\"L\")\n",
        "    elif len(str(i)) == 5:\n",
        "      img = Image.open(\"data/data/0\" + str(i) + \".png\").convert(\"L\")\n",
        "    #transform imaginea in array numpy\n",
        "    img_array = np.array(img)\n",
        "    #adaug imaginea la lista finala\n",
        "    lst.append(img_array)\n",
        "  return lst\n",
        "\n",
        "#creare imaginile de antrenare, care stiu ca sunt de la 1 la 15000\n",
        "train_images = np.array(process_images(1,15001))\n",
        "train_images = train_images.reshape(15000, 50176)\n",
        "\n",
        "#creare imaginile de validare, in numar de 2000, de la 15001 la 17000\n",
        "validation_images = np.array(process_images(15001,17001))\n",
        "validation_images = validation_images.reshape(2000, 50176)\n",
        "\n",
        "#creare imaginile de testare, 5149 care au mai ramas\n",
        "test_images = np.array(process_images(17001,22150))\n",
        "test_images = test_images.reshape(5149, 50176)\n",
        "\n",
        "def process_labels(start, end, file):\n",
        "  labels = np.array([])\n",
        "  with open(file, 'r') as fin:\n",
        "    #sar peste prima linie cu id,class\n",
        "    fin.readline()\n",
        "    for i in range(start,end):\n",
        "      line = [word for word in fin.readline().strip().split(\",\")]\n",
        "      #adaug clasa citita la labels, in format int\n",
        "      labels = np.append(labels, int(line[1]))\n",
        "  return labels\n",
        "\n",
        "#creare labels de antrenare\n",
        "train_labels = process_labels(1,15001,'/content/data/train_labels.txt')\n",
        "\n",
        "#creare labels de validare\n",
        "validation_labels = process_labels(0,2000, '/content/data/validation_labels.txt')\n",
        "\n",
        "dict_scores = {}\n",
        "\n",
        "#metoda values_to_bins preluata din laborator unde impart valorile continue in intervale\n",
        "def values_to_bins(x, bins):\n",
        "    return np.digitize(x, bins) - 1\n",
        "\n",
        "#functia care imi ploteaza graficul cu acuratetea in functie de numarul de bins\n",
        "#asa mi-am dat seama ca folosirea a 4 bins este optima\n",
        "def create_data():\n",
        "  #lista unde retin nr de bins-uri incercate\n",
        "  no_of_bins = []\n",
        "  #lista unde retin ce acuratete am avut in functie de nr de bins\n",
        "  accuracies = []\n",
        "\n",
        "  #testez de la 3 pana la 19 bins\n",
        "  for i in range(3,20):\n",
        "    bins = np.linspace(0, 255, num=i)\n",
        "    no_of_bins.append(i)\n",
        "\n",
        "    #conversia pixelilor in intervalele in care se afla\n",
        "    train = values_to_bins(train_images, bins)\n",
        "    test = values_to_bins(validation_images, bins)\n",
        "\n",
        "    #instantierea modelului\n",
        "    naive_bayes_model = MultinomialNB()\n",
        "    #antrenarea lui\n",
        "    naive_bayes_model.fit(train, train_labels)\n",
        "    #adaugarea acuratetei obtinute in lista\n",
        "    accuracies.append(naive_bayes_model.score(test, validation_labels))\n",
        "    #asocierea acuratetei obtinute pentru fiecare numar de bins incercat\n",
        "    dict_scores[i] = accuracies[-1]\n",
        "\n",
        "  return no_of_bins, accuracies, dict_scores\n",
        "\n",
        "returned_values = create_data()\n",
        "\n",
        "#plotez graficul, pe axa Ox am numarul de bins, pe axa Oy acuratetea obtinuta\n",
        "plt.plot(returned_values[0], returned_values[1])\n",
        "plt.xlim(returned_values[0][0], returned_values[0][-1])\n",
        "plt.show()\n",
        "\n",
        "#in variabila number_bins voi retine numarul de bins pentru care am obtinut acuratea maxima,\n",
        "#folosindu-ma de dictionarul creat anterior\n",
        "for cheie in dict_scores:\n",
        "  if dict_scores[cheie] == max(dict_scores.values()):\n",
        "    number_bins = cheie\n",
        "    break\n",
        "\n",
        "#instantiez modelul\n",
        "naive_bayes_model = MultinomialNB()\n",
        "\n",
        "#creare bins\n",
        "bins = np.linspace(0, 255, num=number_bins)\n",
        "\n",
        "#conversia indecsilor in intervalele in care se afla\n",
        "train_images = values_to_bins(train_images, bins)\n",
        "validation_images = values_to_bins(validation_images, bins)\n",
        "test_images = values_to_bins(test_images, bins)\n",
        "\n",
        "#antrenare\n",
        "naive_bayes_model.fit(train_images, train_labels)\n",
        "\n",
        "#predictii pentru validation_images pentru crearea matricii de confuzie\n",
        "pred = naive_bayes_model.predict(validation_images)\n",
        "\n",
        "#plotarea matricii de confuzie\n",
        "display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(validation_labels, pred), display_labels = [0,1])\n",
        "display.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"Scor: \")\n",
        "print(metrics.accuracy_score(validation_labels, pred))\n",
        "\n",
        "print(\"Precizie clasa 0: \")\n",
        "print(metrics.precision_score(validation_labels, pred, pos_label=0))\n",
        "\n",
        "print(\"Recall clasa 0: \")\n",
        "print(metrics.recall_score(validation_labels, pred, pos_label=0))\n",
        "\n",
        "print(\"Precizie clasa 1: \")\n",
        "print(metrics.precision_score(validation_labels, pred, pos_label=1))\n",
        "\n",
        "print(\"Recall clasa 1: \")\n",
        "print(metrics.recall_score(validation_labels, pred, pos_label=1))\n",
        "\n",
        "#functia pentru fisierul de output\n",
        "def create_output_file(test_images):\n",
        "  #tot aici obtin si predictiile pentru imaginile de test\n",
        "  predictions = naive_bayes_model.predict(test_images)\n",
        "  with open('/content/data/submission.csv', 'w') as fout:\n",
        "    print(\"id,class\", file = fout)\n",
        "    #incep de la 17001, prima imagine de test\n",
        "    image_idx = 17001\n",
        "    for prediction in predictions:\n",
        "      print(\"0\"+str(image_idx)+\",\"+str(int(prediction)), file = fout)\n",
        "      image_idx += 1\n",
        "\n",
        "create_output_file(test_images)"
      ]
    }
  ]
}